# -*- coding: utf-8 -*-
"""
Generate 1000 Bangla questions across 13 namjari tags using OpenAI gpt-5.
Final production version with streaming display and proper CSV output.
"""

import csv
import os
import time
from collections import defaultdict
from openai import OpenAI

# Configuration
MODEL = "gpt-5"
TARGET_ROWS = 1000
OUTPUT_DIR = "namjari_questions"
MAX_COMPLETION_TOKENS = 25000  # Reasonable limit per tag: ~15k reasoning + ~10k output

# Seed data for 13 tags
SEED_DATA = {
    "namjari_process": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦¸à§‡à¦¬à¦¾ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦ªà§‡à¦¤à§‡ à¦ªà¦¾à¦°à¦¿?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦¸à§‡à¦¬à¦¾ à¦ªà§‡à¦¤à§‡ à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?", 
        "à¦–à¦¾à¦°à¦¿à¦œ à¦•à¦°à¦¤à§‡ à¦šà¦¾à¦‡, à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦…à¦¨à¦²à¦¾à¦‡à¦¨à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦®à¦¿à¦‰à¦Ÿà§‡à¦¶à¦¨ à¦•à¦°à¦¤à§‡ à¦¹à¦²à§‡ à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?"
    ],
    "namjari_application_procedure": [
        "à¦†à¦®à¦¿ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¬à§‹ à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦†à¦¬à§‡à¦¦à¦¨ à¦•à¦¿ à¦¨à¦¿à¦œà§‡ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¿à¥¤",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦­à§‚à¦®à¦¿ à¦…à¦«à¦¿à¦¸à§‡ à¦¯à¦¾à¦“à¦¯à¦¼à¦¾ à¦²à¦¾à¦—à§‡?",
        "à¦†à¦®à¦¿ à¦¨à¦¿à¦œà§‡ à¦•à¦¿ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‹?",
        "à¦•à¦¾à¦°à§‹ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯à§‡ à¦•à¦¿ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡?"
    ],
    "namjari_registration": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦œà¦¨à§à¦¯ à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦²à¦¾à¦—à§‡?",
        "à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦•à¦¿ à¦¦à¦°à¦•à¦¾à¦°?",
        "à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦¦à¦²à¦¿à¦² à¦²à¦¾à¦—à§‡?",
        "à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦¨à¦¿à¦œà§‡à¦° à¦®à§‹à¦¬à¦¾à¦‡à¦² à¦¥à¦¾à¦•à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦à¦¨à¦†à¦‡à¦¡à¦¿ à¦¬à¦¾ à¦œà¦¨à§à¦®à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦²à¦¾à¦—à§‡ à¦•à¦¿?"
    ],
    "namjari_by_representative": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦†à¦¬à§‡à¦¦à¦¨ à¦¨à¦¿à¦œà§‡ à¦¨à¦¾ à¦•à¦°à¦²à§‡ à¦ªà§à¦°à¦¤à¦¿à¦¨à¦¿à¦§à¦¿ à¦¦à¦¿à¦¯à¦¼à§‡ à¦•à¦°à¦¾ à¦¯à¦¾à¦¯à¦¼ à¦•à¦¿à¦¨à¦¾?",
        "à¦†à¦®à¦¿ à¦¬à¦¿à¦¦à§‡à¦¶à§‡ à¦¥à¦¾à¦•à¦¿ à¦†à¦®à¦¾à¦° à¦­à¦¾à¦‡ à¦¬à¦¾ à¦†à¦¤à§à¦®à§€à¦¯à¦¼ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦†à¦¬à§‡à¦¦à¦¨ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦•à¦¿à¦¨à¦¾?",
        "à¦ªà§à¦°à¦¤à¦¿à¦¨à¦¿à¦§à¦¿ à¦¦à¦¿à¦¯à¦¼à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾à¦²à§‡ à¦•à¦¿ à¦¤à¦¾à¦° à¦¨à¦¾à¦®à§‡ à¦œà¦®à¦¿ à¦¹à¦¯à¦¼à§‡ à¦¯à¦¾à¦¬à§‡?",
        "à¦†à¦®à¦¿ à¦¥à¦¾à¦•à¦¿à¦¨à¦¾, à¦…à¦¨à§à¦¯ à¦•à§‡à¦‰ à¦•à¦¿ à¦†à¦®à¦¾à¦° à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‡?"
    ],
    "namjari_eligibility": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦–à¦¨ à¦•à¦°à¦¤à§‡ à¦¹à¦¯à¦¼?",
        "à¦†à¦®à¦¿ à¦œà¦®à¦¿ à¦•à¦¿à¦¨à§‡à¦›à¦¿ à¦†à¦®à¦¾à¦° à¦•à¦¿ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾ à¦²à¦¾à¦—à¦¬à§‡?", 
        "à¦†à¦®à¦¿ à¦¦à¦²à¦¿à¦² à¦•à¦°à§‡ à¦œà¦®à¦¿ à¦ªà¦¾à¦‡à¦›à¦¿, à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡?",
        "à¦†à¦®à¦¾à¦° à¦ªà¦¿à¦¤à¦¾ à¦®à¦¾à¦°à¦¾ à¦—à§‡à¦›à§‡à¦¨ à¦†à¦®à¦¿ à¦¤à§‹ à¦œà¦®à¦¿ à¦–à¦¾à¦‡, à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡ à¦•à¦¿?"
    ],
    "namjari_required_documents": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦•à¦¿ à¦¦à¦²à¦¿à¦² à¦²à¦¾à¦—à§‡?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦¦à¦²à¦¿à¦² à¦›à¦¾à¦¡à¦¼à¦¾ à¦†à¦° à¦•à¦¿ à¦²à¦¾à¦—à§‡?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦›à¦¬à¦¿ à¦†à¦° à¦à¦¨à¦†à¦‡à¦¡à¦¿ à¦²à¦¾à¦—à§‡ à¦•à¦¿à¦¨à¦¾?",
        "à¦¦à¦²à¦¿à¦²à§‡à¦° à¦«à¦Ÿà§‹à¦•à¦ªà¦¿ à¦¦à¦¿à¦¯à¦¼à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦¹à¦¬à§‡ à¦•à¦¿à¦¨à¦¾?"
    ],
    "namjari_inheritance_documents": [
        "à¦“à¦¯à¦¼à¦¾à¦°à¦¿à¦¶ à¦®à¦¤à§‡ à¦¨à¦¾à¦® à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¿à¦•à¦¿ à¦•à¦¾à¦—à¦œ à¦²à¦¾à¦˜à§‡?",
        "à¦“à¦¯à¦¼à¦¾à¦°à¦¿à¦¶ à¦®à¦¤à§‡ à¦…à¦¨ à¦²à¦¾à¦‡à¦¨à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾ à¦¯à¦¾à¦¯à¦¼ à¦•à¦¿à¦¨à¦¾?",
        "à¦†à¦®à¦¾à¦° à¦¬à¦¾à¦¬à¦¾ à¦®à¦¾à¦°à¦¾ à¦—à§‡à¦›à§‡à¦¨, à¦à¦–à¦¨ à¦†à¦®à¦¾à¦¦à§‡à¦° à¦¨à¦¾à¦®à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡ à¦•à¦¿?",
        "à¦¬à¦¨à§à¦Ÿà¦¨à¦¨à¦¾à¦®à¦¾ à¦¨à¦¾à¦‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡ à¦•à¦¿?"
    ],
    "namjari_fee": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦¸à¦°à¦•à¦¾à¦°à¦¿ à¦«à¦¿ à¦•à¦¤?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¤ à¦Ÿà¦¾à¦•à¦¾ à¦²à¦¾à¦—à§‡?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¤ à¦–à¦°à¦š à¦¹à¦¬à§‡?",
        "à¦à¦•à¦¾à¦§à¦¿à¦• à¦†à¦¬à§‡à¦¦à¦¨à§‡à¦° à¦œà¦¨à§à¦¯ à¦•à¦¿ à¦¬à§‡à¦¶à¦¿ à¦Ÿà¦¾à¦•à¦¾ à¦²à¦¾à¦—à§‡?"
    ],
    "namjari_hearing_notification": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦†à¦¬à§‡à¦¦à¦¨ à¦¦à¦¾à¦–à¦¿à¦²à§‡à¦° à¦ªà¦° à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦œà¦¨à§à¦¯ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦œà¦¾à¦¨à¦¬à§‹?",
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦¨à§‹à¦Ÿà¦¿à¦¶ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦ªà¦¾à¦¬à§‹?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦œà¦¨à§à¦¯ à¦®à§‹à¦¬à¦¾à¦‡à¦² à¦•à¦°à§‡ à¦–à§‹à¦œ à¦¨à§‡à¦¬ à¦•à¦¿à¦¨à¦¾?",
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦¨à§‹à¦Ÿà¦¿à¦¶à§‡à¦° à¦œà¦¨à§à¦¯ à¦ªà§‹à¦¸à§à¦Ÿ à¦…à¦«à¦¿à¦¸à§‡ à¦–à§‹à¦à¦œ à¦¨à¦¿à¦¤à§‡ à¦¹à¦¬à§‡ à¦•à¦¿?"
    ],
    "namjari_hearing_documents": [
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦œà¦¨à§à¦¯ à¦¸à¦¬ à¦•à¦¾à¦—à¦œ à¦•à¦¿ à¦¨à¦¿à¦¯à¦¼à§‡ à¦¯à§‡à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦œà¦¨à§à¦¯ à¦•à¦¿ à¦•à¦¿ à¦•à¦¾à¦—à¦œà¦ªà¦¤à§à¦° à¦¨à¦¿à¦¯à¦¼à§‡ à¦¯à¦¾à¦¬à§‹?",
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦¸à¦®à¦¯à¦¼ à¦•à¦¿ à¦†à¦¬à§‡à¦¦à¦¨à¦•à¦¾à¦°à§€à¦•à§‡à¦‡ à¦¯à§‡à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦¤à§‡ à¦¸à¦•à¦² à¦†à¦¬à§‡à¦¦à¦¨à¦•à¦¾à¦°à§€à¦•à§‡ à¦¯à§‡à¦¤à§‡ à¦¹à¦¬à§‡ à¦¨à¦¾à¦¤à§‹?"
    ],
    "namjari_status_check": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦¾à¦®à¦²à¦¾ à¦•à¦¿ à¦…à¦¬à¦¸à§à¦¥à¦¾à¦¯à¦¼ à¦†à¦›à§‡ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦œà¦¾à¦¨à¦¬à§‹?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦†à¦¬à§‡à¦¦à¦¨ à¦•à¦°à§‡à¦›à¦¿ à¦…à¦¨à§‡à¦•à¦¦à¦¿à¦¨ à¦•à¦¿ à¦†à¦¬à¦¸à§à¦¥à¦¾à¦¯à¦¼ à¦†à¦›à§‡ à¦œà¦¾à¦¨à¦¬à§‹ à¦•à¦¿à¦­à¦¾à¦¬à§‡?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦¾à¦®à¦²à¦¾à¦° à¦¸à§à¦Ÿà§à¦¯à¦¾à¦Ÿà¦¾à¦¸ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦œà¦¾à¦¨à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‹?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦¾à¦®à¦²à¦¾à¦° à¦…à¦¬à¦¸à§à¦¥à¦¾ à¦œà¦¾à¦¨à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦•à¦¿?"
    ],
    "namjari_rejected_appeal": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦¾à¦®à¦²à¦¾ à¦¨à¦¾à¦®à¦à§à¦œà§à¦° à¦¹à¦²à§‡ à¦•à¦¿ à¦•à¦°à¦¬à§‹?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦¾à¦®à¦²à¦¾ à¦¨à¦¾à¦®à¦à§à¦œà§à¦° à¦¹à¦¯à¦¼à§‡à¦›à§‡ à¦•à§‹à¦¨ à¦†à¦¦à¦¾à¦²à¦¤à§‡ à¦¯à¦¾à¦¬à§‹?", 
        "à¦¨à¦¾à¦®à¦à§à¦œà§à¦° à¦¹à¦²à§‡ à¦†à¦¬à¦¾à¦° à¦•à¦¿ à¦†à¦¬à§‡à¦¦à¦¨ à¦•à¦°à¦¬à§‹?",
        "à¦à¦¸à¦¿ (à¦²à§à¦¯à¦¾à¦¨à§à¦¡) à¦¶à§à¦¨à¦¾à¦¨à§€ à¦¨à¦¾ à¦¨à¦¿à¦¯à¦¼à§‡ à¦¨à¦¾à¦®à¦à§à¦œà§à¦° à¦•à¦°à§‡à¦›à§‡à¦¨?"
    ],
    "namjari_khatian_copy": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦à§à¦œà§à¦° à¦¹à¦¯à¦¼à§‡à¦›à§‡ à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨ à¦•à¦ªà¦¿ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦ªà§‡à¦¤à§‡ à¦ªà¦¾à¦°à¦¿?",
        "à¦†à¦®à¦¾à¦° à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦à§à¦œà§à¦° à¦¹à¦¯à¦¼à§‡à¦›à§‡ à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨ à¦•à¦¿ à¦†à¦®à¦¿ à¦‰à¦ à¦¾à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‹?",
        "à¦…à¦¨à¦²à¦¾à¦‡à¦¨à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à§‡à¦›à¦¿ à¦†à¦®à¦¾à¦° à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨à§‡à¦° à¦•à¦ªà¦¿ à¦•à§‹à¦¥à¦¾à¦¯à¦¼ à¦ªà¦¾à¦¬à§‹?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦à§à¦œà§à¦° à¦¹à¦²à§‡ à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨à§‡à¦° à¦ªà§à¦°à¦¿à¦¨à§à¦Ÿ à¦•à¦ªà¦¿ à¦¨à¦¿à¦œà§‡ à¦•à¦¿ à¦¨à¦¿à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‹?"
    ],
    "namjari_khatian_correction": [
        "à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨à§‡ à¦­à§à¦² à¦†à¦›à§‡ à¦¸à¦‚à¦¶à§‹à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨à§‡ à¦¨à¦¾à¦® à¦­à§à¦² à¦²à§‡à¦–à¦¾ à¦¸à¦‚à¦¶à§‹à¦§à¦¨ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦•à¦°à¦¬à§‹?",
        "à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨à§‡ à¦œà¦®à¦¿à¦° à¦ªà¦°à¦¿à¦®à¦¾à¦£ à¦­à§à¦² à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦¸à¦‚à¦¶à§‹à¦§à¦¨ à¦•à¦°à¦¬à§‹?",
        "à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨ à¦¸à¦‚à¦¶à§‹à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦•à¦¿ à¦•à¦¾à¦—à¦œ à¦²à¦¾à¦—à§‡?",
        "à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨ à¦¸à¦‚à¦¶à§‹à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¤ à¦Ÿà¦¾à¦•à¦¾ à¦²à¦¾à¦—à§‡?"
    ]
}

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def clean_question(text):
    """Basic cleaning of question text."""
    return text.strip().replace('"', '').replace("'", '')

def get_cross_tag_exclusions(current_tag):
    """Generate explicit exclusion rules with concrete examples to prevent cross-tag contamination."""
    
    # Build exclusion rules dynamically using actual SEED_DATA
    exclusion_rules = []
    
    if current_tag == 'namjari_process':
        exclusion_rules.extend([
            "âŒ DON'T generate document questions like these (belongs to namjari_required_documents):",
        ])
        for example in SEED_DATA['namjari_required_documents'][:1]:  # Show only 1 example for efficiency
            exclusion_rules.append(f"   â€¢ \"{example}\"")
        
        exclusion_rules.extend([
            "âŒ DON'T generate fee questions like these (belongs to namjari_fee):",
        ])
        for example in SEED_DATA['namjari_fee'][:1]:  # Show only 1 example for efficiency
            exclusion_rules.append(f"   â€¢ \"{example}\"")
        
        exclusion_rules.extend([
            "âŒ DON'T generate inheritance questions like these (belongs to namjari_inheritance_documents):",
        ])
        for example in SEED_DATA['namjari_inheritance_documents'][:1]:  # Show only 1 example for efficiency
            exclusion_rules.append(f"   â€¢ \"{example}\"")
        
        exclusion_rules.extend([
            "âŒ DON'T generate status questions like these (belongs to namjari_status_check):",
        ])
        for example in SEED_DATA['namjari_status_check'][:1]:  # Show only 1 example for efficiency
            exclusion_rules.append(f"   â€¢ \"{example}\"")
        
        exclusion_rules.append("âŒ ONLY generate general process questions like your examples!")
    
    elif current_tag == 'namjari_application_procedure':
        exclusion_rules.extend([
            "âŒ DON'T generate document questions like these (belongs to namjari_required_documents):",
        ])
        for example in SEED_DATA['namjari_required_documents']:
            exclusion_rules.append(f"   â€¢ \"{example}\"")
        
        exclusion_rules.extend([
            "âŒ DON'T generate representative questions like these (belongs to namjari_by_representative):",
        ])
        for example in SEED_DATA['namjari_by_representative']:
            exclusion_rules.append(f"   â€¢ \"{example}\"")
        
        exclusion_rules.extend([
            "âŒ DON'T generate fee questions like these (belongs to namjari_fee):",
        ])
        for example in SEED_DATA['namjari_fee']:
            exclusion_rules.append(f"   â€¢ \"{example}\"")
        
        exclusion_rules.append("âŒ ONLY generate self-capability questions with 'à¦¨à¦¿à¦œà§‡', 'à¦†à¦®à¦¿ à¦¨à¦¿à¦œà§‡' patterns!")
    
    elif current_tag == 'namjari_fee':
        exclusion_rules.extend([
            "âŒ DON'T generate process questions like these (belongs to namjari_process):",
        ])
        for example in SEED_DATA['namjari_process']:
            exclusion_rules.append(f"   â€¢ \"{example}\"")
        
        exclusion_rules.extend([
            "âŒ DON'T generate document questions like these (belongs to namjari_required_documents):",
        ])
        for example in SEED_DATA['namjari_required_documents']:
            exclusion_rules.append(f"   â€¢ \"{example}\"")
        
        exclusion_rules.extend([
            "âŒ DON'T generate hearing questions like these (belongs to namjari_hearing_notification):",
        ])
        for example in SEED_DATA['namjari_hearing_notification']:
            exclusion_rules.append(f"   â€¢ \"{example}\"")
        
        exclusion_rules.append("âŒ ONLY generate cost/fee questions with 'à¦«à¦¿', 'à¦Ÿà¦¾à¦•à¦¾', 'à¦–à¦°à¦š', 'à¦¸à¦°à¦•à¦¾à¦°à¦¿'!")
    
    else:
        # For other tags, create a more generic exclusion pattern
        excluded_tags = [tag for tag in SEED_DATA.keys() if tag != current_tag][:3]  # Show top 3 most different tags
        for excluded_tag in excluded_tags:
            exclusion_rules.append(f"âŒ DON'T generate questions like these (belongs to {excluded_tag}):")
            for example in SEED_DATA[excluded_tag][:1]:  # Show 1 example per excluded tag for efficiency
                exclusion_rules.append(f"   â€¢ \"{example}\"")
        
        exclusion_rules.append(f"âŒ ONLY generate questions that fit {current_tag} domain!")
    
    return "\n".join(exclusion_rules) if exclusion_rules else "âŒ Stay strictly within this tag's domain"

def analyze_question_patterns(seed_questions, tag):
    """Advanced pattern analysis capturing vocabulary, tone, context, and distinctions."""
    
    # Tag-specific vocabulary mapping
    tag_vocabularies = {
        'namjari_process': ['à¦¸à§‡à¦¬à¦¾', 'à¦–à¦¾à¦°à¦¿à¦œ', 'à¦®à¦¿à¦‰à¦Ÿà§‡à¦¶à¦¨', 'à¦…à¦¨à¦²à¦¾à¦‡à¦¨à§‡'],
        'namjari_application_procedure': ['à¦¨à¦¿à¦œà§‡', 'à¦­à§‚à¦®à¦¿ à¦…à¦«à¦¿à¦¸', 'à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯à§‡', 'à¦†à¦®à¦¿ à¦¨à¦¿à¦œà§‡'],
        'namjari_registration': ['à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨', 'à¦®à§‹à¦¬à¦¾à¦‡à¦²', 'à¦à¦¨à¦†à¦‡à¦¡à¦¿', 'à¦œà¦¨à§à¦®à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨'],
        'namjari_by_representative': ['à¦ªà§à¦°à¦¤à¦¿à¦¨à¦¿à¦§à¦¿', 'à¦¬à¦¿à¦¦à§‡à¦¶à§‡', 'à¦­à¦¾à¦‡', 'à¦†à¦¤à§à¦®à§€à¦¯à¦¼'],
        'namjari_eligibility': ['à¦•à¦–à¦¨', 'à¦œà¦®à¦¿ à¦•à¦¿à¦¨à§‡à¦›à¦¿', 'à¦ªà¦¿à¦¤à¦¾ à¦®à¦¾à¦°à¦¾ à¦—à§‡à¦›à§‡à¦¨', 'à¦¦à¦²à¦¿à¦² à¦•à¦°à§‡'],
        'namjari_required_documents': ['à¦¦à¦²à¦¿à¦²', 'à¦›à¦¬à¦¿', 'à¦à¦¨à¦†à¦‡à¦¡à¦¿', 'à¦«à¦Ÿà§‹à¦•à¦ªà¦¿'],
        'namjari_inheritance_documents': ['à¦“à¦¯à¦¼à¦¾à¦°à¦¿à¦¶', 'à¦¬à¦¾à¦¬à¦¾ à¦®à¦¾à¦°à¦¾ à¦—à§‡à¦›à§‡à¦¨', 'à¦¬à¦¨à§à¦Ÿà¦¨à¦¨à¦¾à¦®à¦¾'],
        'namjari_fee': ['à¦«à¦¿', 'à¦Ÿà¦¾à¦•à¦¾', 'à¦–à¦°à¦š', 'à¦¸à¦°à¦•à¦¾à¦°à¦¿'],
        'namjari_hearing_notification': ['à¦¶à§à¦¨à¦¾à¦¨à§€', 'à¦¨à§‹à¦Ÿà¦¿à¦¶', 'à¦®à§‹à¦¬à¦¾à¦‡à¦² à¦•à¦°à§‡', 'à¦ªà§‹à¦¸à§à¦Ÿ à¦…à¦«à¦¿à¦¸'],
        'namjari_hearing_documents': ['à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦œà¦¨à§à¦¯', 'à¦•à¦¾à¦—à¦œà¦ªà¦¤à§à¦°', 'à¦†à¦¬à§‡à¦¦à¦¨à¦•à¦¾à¦°à§€'],
        'namjari_status_check': ['à¦®à¦¾à¦®à¦²à¦¾', 'à¦…à¦¬à¦¸à§à¦¥à¦¾à¦¯à¦¼', 'à¦¸à§à¦Ÿà§à¦¯à¦¾à¦Ÿà¦¾à¦¸', 'à¦œà¦¾à¦¨à¦¬à§‹ à¦•à¦¿à¦­à¦¾à¦¬à§‡'],
        'namjari_rejected_appeal': ['à¦¨à¦¾à¦®à¦à§à¦œà§à¦°', 'à¦†à¦¦à¦¾à¦²à¦¤', 'à¦à¦¸à¦¿ (à¦²à§à¦¯à¦¾à¦¨à§à¦¡)', 'à¦†à¦¬à¦¾à¦° à¦†à¦¬à§‡à¦¦à¦¨'],
        'namjari_khatian_copy': ['à¦®à¦à§à¦œà§à¦°', 'à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨ à¦•à¦ªà¦¿', 'à¦‰à¦ à¦¾à¦¤à§‡', 'à¦ªà§à¦°à¦¿à¦¨à§à¦Ÿ à¦•à¦ªà¦¿'],
        'namjari_khatian_correction': ['à¦­à§à¦²', 'à¦¸à¦‚à¦¶à§‹à¦§à¦¨', 'à¦¨à¦¾à¦® à¦­à§à¦²', 'à¦œà¦®à¦¿à¦° à¦ªà¦°à¦¿à¦®à¦¾à¦£']
    }
    
    # Tag-specific contexts and tones
    tag_contexts = {
        'namjari_process': 'General service inquiry tone - seeking basic information',
        'namjari_application_procedure': 'Self-capability concern tone - "à¦†à¦®à¦¿ à¦¨à¦¿à¦œà§‡" patterns',
        'namjari_registration': 'Technical requirement tone - system setup focus',
        'namjari_by_representative': 'Delegation concern tone - long conditional questions',
        'namjari_eligibility': 'Situational qualification tone - life event contexts',
        'namjari_required_documents': 'Document-focused tone - practical requirements',
        'namjari_inheritance_documents': 'Emotional family tone - death/inheritance context',
        'namjari_fee': 'Cost-conscious tone - purely financial focus',
        'namjari_hearing_notification': 'Information-seeking tone - "à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦œà¦¾à¦¨à¦¬à§‹" patterns',
        'namjari_hearing_documents': 'Preparation-focused tone - "à¦•à¦¿ à¦¨à¦¿à¦¯à¦¼à§‡ à¦¯à¦¾à¦¬à§‹" patterns',
        'namjari_status_check': 'Anxious follow-up tone - tracking progress',
        'namjari_rejected_appeal': 'Problem-solving tone - dealing with rejection',
        'namjari_khatian_copy': 'Success-phase tone - getting final documents',
        'namjari_khatian_correction': 'Error-fixing tone - correcting mistakes'
    }
    
    # Extract actual patterns from seed questions
    question_starters = []
    structures = []
    key_phrases = []
    
    for q in seed_questions:
        # Question starter analysis
        if q.startswith('à¦•à¦¿à¦­à¦¾à¦¬à§‡'):
            question_starters.append('à¦•à¦¿à¦­à¦¾à¦¬à§‡')
        elif q.startswith('à¦•à¦¿ '):
            question_starters.append('à¦•à¦¿')
        elif q.startswith('à¦•à¦¤'):
            question_starters.append('à¦•à¦¤')
        elif q.startswith('à¦•à§‹à¦¥à¦¾à¦¯à¦¼'):
            question_starters.append('à¦•à§‹à¦¥à¦¾à¦¯à¦¼')
        elif q.startswith('à¦†à¦®à¦¿'):
            question_starters.append('à¦†à¦®à¦¿')
        
        # Structure analysis
        if 'à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡' in q:
            structures.append('X à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?')
        if 'à¦•à¦¿à¦­à¦¾à¦¬à§‡' in q and 'à¦ªà¦¾à¦°à¦¿' in q:
            structures.append('X à¦•à¦¿à¦­à¦¾à¦¬à§‡ Y à¦ªà¦¾à¦°à¦¿?')
        if 'à¦•à¦¿à¦­à¦¾à¦¬à§‡' in q and 'à¦ªà¦¾à¦¬à§‹' in q:
            structures.append('X à¦•à¦¿à¦­à¦¾à¦¬à§‡ Y à¦ªà¦¾à¦¬à§‹?')
        if 'à¦•à¦¤' in q and 'à¦²à¦¾à¦—à§‡' in q:
            structures.append('à¦•à¦¤ X à¦²à¦¾à¦—à§‡?')
        
        # Extract unique phrases
        for phrase in tag_vocabularies.get(tag, []):
            if phrase in q:
                key_phrases.append(phrase)
    
    analysis = f"""
ADVANCED PATTERN ANALYSIS FOR {tag.upper()}:

ğŸ¯ TAG-SPECIFIC CONTEXT: {tag_contexts.get(tag, 'Standard namjari context')}

ğŸ—£ï¸ QUESTION STARTERS: {', '.join(set(question_starters)) if question_starters else 'Mixed'}
ğŸ“ SENTENCE STRUCTURES: {', '.join(set(structures)) if structures else 'Varied'}
ğŸ’¬ CRITICAL VOCABULARY: {', '.join(set(key_phrases)) if key_phrases else 'Standard'}
ğŸ“Š EXPECTED VOCABULARY: {', '.join(tag_vocabularies.get(tag, ['à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿']))}

âš¡ GENERATION RULES FOR THIS TAG:
1. MUST use the exact vocabulary: {', '.join(tag_vocabularies.get(tag, ['à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿']))}
2. MUST match the tone: {tag_contexts.get(tag, 'Standard')}
3. MUST follow structures: {', '.join(set(structures)) if structures else 'Same as examples'}
4. MUST start questions like examples: {', '.join(set(question_starters)) if question_starters else 'Varied starters'}

â­ CRITICAL: This tag is DISTINCT from all others - maintain its unique vocabulary and context!"""
    
    return analysis

def display_generated_text_streaming(generated_text, tag):
    """Display generated text with streaming effect and return cleaned lines."""
    print(f"ğŸ“º Generated content for {tag}:")
    print("=" * 80)
    
    lines = generated_text.strip().split('\n')
    cleaned_lines = []
    
    for line in lines:
        clean_line = clean_question(line)
        if len(clean_line) > 5 and any('\u0980' <= char <= '\u09FF' for char in clean_line):
            print(f"âœ¨ {clean_line}")
            cleaned_lines.append(clean_line)
            time.sleep(0.1)  # Small delay for streaming effect
    
    print("=" * 80)
    return cleaned_lines

def generate_questions_for_tag(tag, seed_questions, target_count):
    """Generate questions for a tag and stream to file."""
    
    # Create output directory
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    # Create file and write header immediately
    filename = f"{tag}.csv"
    filepath = os.path.join(OUTPUT_DIR, filename)
    
    print(f"\nğŸ·ï¸  Processing: {tag}")
    print(f"ğŸ“ Creating: {filepath}")
    print(f"ğŸ¯ Target: {target_count} questions")
    
    with open(filepath, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f, quoting=csv.QUOTE_ALL)
        writer.writerow(['question', 'tag'])
        
        # Write seed questions first  
        print(f"ğŸ“ Writing {len(seed_questions)} seed questions...")
        for i, seed in enumerate(seed_questions, 1):
            clean_q = clean_question(seed)
            writer.writerow([clean_q, tag])
            f.flush()
            print(f"  {i:2d}. {clean_q}")
        
        seed_count = len(seed_questions)
        print(f"âœ… Wrote {seed_count} seed questions")
        
        # Generate more if needed
        questions_needed = target_count - seed_count
        if questions_needed <= 0:
            print(f"âœ… Target reached with seeds only")
            return filepath
        
        print(f"ğŸ¤– Generating {questions_needed} new questions...")
        
        # Create highly specific prompt for maximum similarity
        examples = "\n".join(seed_questions)
        
        # Analyze patterns in seed questions for this tag
        pattern_analysis = analyze_question_patterns(seed_questions, tag)
        
        prompt = f"""Generate {questions_needed} new Bengali questions that are 97-99% IDENTICAL in style, structure, and vocabulary to these examples:

{examples}

CRITICAL REQUIREMENTS - Follow these EXACTLY:
- Copy the exact sentence structures from examples above
- Use the same question words (à¦•à¦¿à¦­à¦¾à¦¬à§‡, à¦•à¦¿, à¦•à§‹à¦¥à¦¾à¦¯à¦¼, à¦•à¦¤, etc.) as in examples
- Use the same vocabulary and terminology as examples 
- Maintain the exact same colloquial style and tone
- Keep the same question length patterns
- Use the same grammatical patterns
- Replace only minimal words while keeping core structure identical

PATTERN ANALYSIS FOR THIS TAG:
{pattern_analysis}

ğŸš¨ CROSS-TAG CONTAMINATION GUARDRAILS:
NEVER generate questions that could fit these OTHER tags:
{get_cross_tag_exclusions(tag)}

EXAMPLE OF WHAT TO DO:
If example is: "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦¸à§‡à¦¬à¦¾ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦ªà§‡à¦¤à§‡ à¦ªà¦¾à¦°à¦¿?"
Generate like: "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦¸à§‡à¦¬à¦¾ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦¨à¦¿à¦¤à§‡ à¦ªà¦¾à¦°à¦¿?" or "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦¸à§‡à¦¬à¦¾ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦—à§à¦°à¦¹à¦£ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¿?"
(Same structure, same question word, minimal vocabulary change)

Generate EXACTLY {questions_needed} questions following these patterns:"""

        try:
            print("ğŸ”„ Calling OpenAI...")
            
            # Prepare API call parameters
            api_params = {
                "model": MODEL,
                "messages": [
                    {"role": "system", "content": "You are an expert Bengali question generator specializing in land registration (namjari) topics. Your task is to generate questions that are 97-99% IDENTICAL to provided examples in style, structure, vocabulary, and tone. Follow the exact patterns shown in examples. Generate only pure questions, one per line, with no numbering, bullets, or extra text."},
                    {"role": "user", "content": prompt}
                ]
            }
            
            # Add max_completion_tokens only if specified
            if MAX_COMPLETION_TOKENS is not None:
                api_params["max_completion_tokens"] = MAX_COMPLETION_TOKENS
                print(f"ğŸ›ï¸  Using custom token limit: {MAX_COMPLETION_TOKENS}")
            else:
                print("ğŸ›ï¸  Using OpenAI default token limits")
            
            response = client.chat.completions.create(**api_params)
            
            # Extract and display generated text with streaming
            generated_text = response.choices[0].message.content
            print(f"ğŸ“¡ Generated text length: {len(generated_text) if generated_text else 0} characters")
            
            # Debug: print response details if empty
            if not generated_text or len(generated_text.strip()) == 0:
                print(f"âš ï¸  Empty response detected!")
                print(f"ğŸ” Response object: {response}")
                print(f"ğŸ” Response choices: {len(response.choices) if response.choices else 0}")
                if response.choices and len(response.choices) > 0:
                    print(f"ğŸ” First choice: {response.choices[0]}")
                    print(f"ğŸ” Message content: '{response.choices[0].message.content}'")
            
            # Display generated text with streaming effect and get cleaned lines
            cleaned_lines = display_generated_text_streaming(generated_text, tag)
            
            # Add cleaned questions to CSV
            added_count = 0
            print(f"ğŸ“ Writing {len(cleaned_lines)} valid questions to CSV...")
            
            for clean_q in cleaned_lines:
                writer.writerow([clean_q, tag])
                f.flush()
                added_count += 1
                print(f"âœ… Written to CSV: {clean_q[:50]}..." if len(clean_q) > 50 else f"âœ… Written to CSV: {clean_q}")
                
                if seed_count + added_count >= target_count:
                    break
            
            print(f"âœ… Added {added_count} new questions")
            print(f"ğŸ“Š Total in file: {seed_count + added_count}")
            
        except Exception as e:
            print(f"âŒ Error generating questions: {e}")
            print(f"ğŸ“Š File contains {seed_count} seed questions only")
            print(f"ğŸ”„ You can run the script again to retry generation for this tag")
    
    return filepath

def main():
    print("ğŸš€ Namjari Question Generator - PRODUCTION MODE (All 13 Tags)")
    print("=" * 80)
    print(f"ğŸ¯ Target: {TARGET_ROWS} questions across 13 tags")
    print(f"ğŸ“º Will show all generated questions streaming!")
    print(f"ğŸ”§ Model: {MODEL}")
    print(f"ğŸ“ Output: {OUTPUT_DIR}/ (individual files)")
    print("=" * 80)
    
    # Process all tags
    all_tags = list(SEED_DATA.keys())
    tags = all_tags  # All 13 tags for production
    
    # Calculate questions per tag
    base_count = TARGET_ROWS // len(tags)
    remainder = TARGET_ROWS % len(tags)
    
    print(f"ğŸ“Š Distribution: {base_count} questions per tag")
    if remainder > 0:
        print(f"ğŸ“Š Extra: {remainder} tags will get +1 question")
    
    files_created = []
    
    start_time = time.time()
    
    for i, tag in enumerate(tags):
        target = base_count + (1 if i < remainder else 0)
        seeds = SEED_DATA[tag]
        
        print(f"\n{'='*80}")
        print(f"[{i+1}/{len(tags)}] Processing: {tag}")
        print(f"ğŸ¯ Target: {target} questions ({len(seeds)} seeds + {target-len(seeds)} new)")
        print(f"{'='*80}")
        
        tag_start_time = time.time()
        filepath = generate_questions_for_tag(tag, seeds, target)
        tag_duration = time.time() - tag_start_time
        
        # Count actual questions in file
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                actual_count = sum(1 for _ in reader)
        except Exception as e:
            print(f"âš ï¸ Warning: Could not count questions in file: {e}")
            actual_count = len(seeds)
            
        files_created.append((tag, filepath, actual_count, tag_duration))
        
        print(f"ğŸ‰ Completed: {os.path.basename(filepath)} ({actual_count} questions)")
        print(f"â±ï¸  Time taken: {tag_duration:.1f} seconds")
        print(f"ğŸ“¡ Monitor: tail -f {filepath}")
    
    # Summary
    total_duration = time.time() - start_time
    total_questions = sum(count for _, _, count, _ in files_created)
    
    print(f"\nğŸ† ALL DONE!")
    print("=" * 80)
    print(f"ğŸ“ Directory: {OUTPUT_DIR}/")
    print(f"ğŸ“„ Files: {len(files_created)}")
    print(f"ğŸ“Š Total questions: {total_questions}")
    print(f"â±ï¸  Total time: {total_duration:.1f} seconds")
    print(f"âš¡ Average: {total_questions/total_duration:.1f} questions/second")
    print("=" * 80)
    
    print(f"\nğŸ“‹ Detailed Results:")
    print(f"{'Tag':<30} {'File':<35} {'Questions':<10} {'Time(s)':<8}")
    print("-" * 85)
    for tag, filepath, count, duration in files_created:
        filename = os.path.basename(filepath)
        print(f"{tag:<30} {filename:<35} {count:<10} {duration:<8.1f}")
    
    print(f"\nğŸ“¡ Monitor commands:")
    print("   ls -la namjari_questions/")
    print("   wc -l namjari_questions/*.csv")
    print("   find namjari_questions/ -name '*.csv' -exec wc -l {} + | sort -n")
    
    print(f"\nğŸ¯ Mission Accomplished!")
    print(f"   Generated {total_questions} Bengali questions across {len(files_created)} namjari topics!")
    print(f"   Ready for dataset training! ğŸš€")

if __name__ == "__main__":
    main()
