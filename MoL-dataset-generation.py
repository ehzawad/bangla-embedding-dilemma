# -*- coding: utf-8 -*-
"""
Generate 1000 Bangla questions across 13 namjari tags using OpenAI gpt-5.
Final production version with streaming display and proper CSV output.
"""

import csv
import os
import time
from collections import defaultdict
from openai import OpenAI

# Configuration
MODEL = "gpt-5"
TARGET_ROWS = 1000
OUTPUT_DIR = "namjari_questions"

# Seed data for 13 tags
SEED_DATA = {
    "namjari_process": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦¸à§‡à¦¬à¦¾ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦ªà§‡à¦¤à§‡ à¦ªà¦¾à¦°à¦¿?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦¸à§‡à¦¬à¦¾ à¦ªà§‡à¦¤à§‡ à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?", 
        "à¦–à¦¾à¦°à¦¿à¦œ à¦•à¦°à¦¤à§‡ à¦šà¦¾à¦‡, à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦…à¦¨à¦²à¦¾à¦‡à¦¨à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦®à¦¿à¦‰à¦Ÿà§‡à¦¶à¦¨ à¦•à¦°à¦¤à§‡ à¦¹à¦²à§‡ à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?"
    ],
    "namjari_application_procedure": [
        "à¦†à¦®à¦¿ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¬à§‹ à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦†à¦¬à§‡à¦¦à¦¨ à¦•à¦¿ à¦¨à¦¿à¦œà§‡ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¿à¥¤",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦­à§‚à¦®à¦¿ à¦…à¦«à¦¿à¦¸à§‡ à¦¯à¦¾à¦“à¦¯à¦¼à¦¾ à¦²à¦¾à¦—à§‡?",
        "à¦†à¦®à¦¿ à¦¨à¦¿à¦œà§‡ à¦•à¦¿ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‹?",
        "à¦•à¦¾à¦°à§‹ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯à§‡ à¦•à¦¿ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡?"
    ],
    "namjari_registration": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦œà¦¨à§à¦¯ à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦²à¦¾à¦—à§‡?",
        "à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦•à¦¿ à¦¦à¦°à¦•à¦¾à¦°?",
        "à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦¦à¦²à¦¿à¦² à¦²à¦¾à¦—à§‡?",
        "à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦¨à¦¿à¦œà§‡à¦° à¦®à§‹à¦¬à¦¾à¦‡à¦² à¦¥à¦¾à¦•à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦à¦¨à¦†à¦‡à¦¡à¦¿ à¦¬à¦¾ à¦œà¦¨à§à¦®à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦²à¦¾à¦—à§‡ à¦•à¦¿?"
    ],
    "namjari_by_representative": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦†à¦¬à§‡à¦¦à¦¨ à¦¨à¦¿à¦œà§‡ à¦¨à¦¾ à¦•à¦°à¦²à§‡ à¦ªà§à¦°à¦¤à¦¿à¦¨à¦¿à¦§à¦¿ à¦¦à¦¿à¦¯à¦¼à§‡ à¦•à¦°à¦¾ à¦¯à¦¾à¦¯à¦¼ à¦•à¦¿à¦¨à¦¾?",
        "à¦†à¦®à¦¿ à¦¬à¦¿à¦¦à§‡à¦¶à§‡ à¦¥à¦¾à¦•à¦¿ à¦†à¦®à¦¾à¦° à¦­à¦¾à¦‡ à¦¬à¦¾ à¦†à¦¤à§à¦®à§€à¦¯à¦¼ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦†à¦¬à§‡à¦¦à¦¨ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦•à¦¿à¦¨à¦¾?",
        "à¦ªà§à¦°à¦¤à¦¿à¦¨à¦¿à¦§à¦¿ à¦¦à¦¿à¦¯à¦¼à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾à¦²à§‡ à¦•à¦¿ à¦¤à¦¾à¦° à¦¨à¦¾à¦®à§‡ à¦œà¦®à¦¿ à¦¹à¦¯à¦¼à§‡ à¦¯à¦¾à¦¬à§‡?",
        "à¦†à¦®à¦¿ à¦¥à¦¾à¦•à¦¿à¦¨à¦¾, à¦…à¦¨à§à¦¯ à¦•à§‡à¦‰ à¦•à¦¿ à¦†à¦®à¦¾à¦° à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‡?"
    ],
    "namjari_eligibility": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦–à¦¨ à¦•à¦°à¦¤à§‡ à¦¹à¦¯à¦¼?",
        "à¦†à¦®à¦¿ à¦œà¦®à¦¿ à¦•à¦¿à¦¨à§‡à¦›à¦¿ à¦†à¦®à¦¾à¦° à¦•à¦¿ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾ à¦²à¦¾à¦—à¦¬à§‡?", 
        "à¦†à¦®à¦¿ à¦¦à¦²à¦¿à¦² à¦•à¦°à§‡ à¦œà¦®à¦¿ à¦ªà¦¾à¦‡à¦›à¦¿, à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡?",
        "à¦†à¦®à¦¾à¦° à¦ªà¦¿à¦¤à¦¾ à¦®à¦¾à¦°à¦¾ à¦—à§‡à¦›à§‡à¦¨ à¦†à¦®à¦¿ à¦¤à§‹ à¦œà¦®à¦¿ à¦–à¦¾à¦‡, à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡ à¦•à¦¿?"
    ],
    "namjari_required_documents": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦•à¦¿ à¦¦à¦²à¦¿à¦² à¦²à¦¾à¦—à§‡?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦¦à¦²à¦¿à¦² à¦›à¦¾à¦¡à¦¼à¦¾ à¦†à¦° à¦•à¦¿ à¦²à¦¾à¦—à§‡?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦›à¦¬à¦¿ à¦†à¦° à¦à¦¨à¦†à¦‡à¦¡à¦¿ à¦²à¦¾à¦—à§‡ à¦•à¦¿à¦¨à¦¾?",
        "à¦¦à¦²à¦¿à¦²à§‡à¦° à¦«à¦Ÿà§‹à¦•à¦ªà¦¿ à¦¦à¦¿à¦¯à¦¼à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦¹à¦¬à§‡ à¦•à¦¿à¦¨à¦¾?"
    ],
    "namjari_inheritance_documents": [
        "à¦“à¦¯à¦¼à¦¾à¦°à¦¿à¦¶ à¦®à¦¤à§‡ à¦¨à¦¾à¦® à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¿à¦•à¦¿ à¦•à¦¾à¦—à¦œ à¦²à¦¾à¦˜à§‡?",
        "à¦“à¦¯à¦¼à¦¾à¦°à¦¿à¦¶ à¦®à¦¤à§‡ à¦…à¦¨ à¦²à¦¾à¦‡à¦¨à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾ à¦¯à¦¾à¦¯à¦¼ à¦•à¦¿à¦¨à¦¾?",
        "à¦†à¦®à¦¾à¦° à¦¬à¦¾à¦¬à¦¾ à¦®à¦¾à¦°à¦¾ à¦—à§‡à¦›à§‡à¦¨, à¦à¦–à¦¨ à¦†à¦®à¦¾à¦¦à§‡à¦° à¦¨à¦¾à¦®à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡ à¦•à¦¿?",
        "à¦¬à¦¨à§à¦Ÿà¦¨à¦¨à¦¾à¦®à¦¾ à¦¨à¦¾à¦‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡ à¦•à¦¿?"
    ],
    "namjari_fee": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦¸à¦°à¦•à¦¾à¦°à¦¿ à¦«à¦¿ à¦•à¦¤?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¤ à¦Ÿà¦¾à¦•à¦¾ à¦²à¦¾à¦—à§‡?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¤ à¦–à¦°à¦š à¦¹à¦¬à§‡?",
        "à¦à¦•à¦¾à¦§à¦¿à¦• à¦†à¦¬à§‡à¦¦à¦¨à§‡à¦° à¦œà¦¨à§à¦¯ à¦•à¦¿ à¦¬à§‡à¦¶à¦¿ à¦Ÿà¦¾à¦•à¦¾ à¦²à¦¾à¦—à§‡?"
    ],
    "namjari_hearing_notification": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦†à¦¬à§‡à¦¦à¦¨ à¦¦à¦¾à¦–à¦¿à¦²à§‡à¦° à¦ªà¦° à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦œà¦¨à§à¦¯ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦œà¦¾à¦¨à¦¬à§‹?",
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦¨à§‹à¦Ÿà¦¿à¦¶ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦ªà¦¾à¦¬à§‹?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦œà¦¨à§à¦¯ à¦®à§‹à¦¬à¦¾à¦‡à¦² à¦•à¦°à§‡ à¦–à§‹à¦œ à¦¨à§‡à¦¬ à¦•à¦¿à¦¨à¦¾?",
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦¨à§‹à¦Ÿà¦¿à¦¶à§‡à¦° à¦œà¦¨à§à¦¯ à¦ªà§‹à¦¸à§à¦Ÿ à¦…à¦«à¦¿à¦¸à§‡ à¦–à§‹à¦à¦œ à¦¨à¦¿à¦¤à§‡ à¦¹à¦¬à§‡ à¦•à¦¿?"
    ],
    "namjari_hearing_documents": [
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦œà¦¨à§à¦¯ à¦¸à¦¬ à¦•à¦¾à¦—à¦œ à¦•à¦¿ à¦¨à¦¿à¦¯à¦¼à§‡ à¦¯à§‡à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦œà¦¨à§à¦¯ à¦•à¦¿ à¦•à¦¿ à¦•à¦¾à¦—à¦œà¦ªà¦¤à§à¦° à¦¨à¦¿à¦¯à¦¼à§‡ à¦¯à¦¾à¦¬à§‹?",
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦° à¦¸à¦®à¦¯à¦¼ à¦•à¦¿ à¦†à¦¬à§‡à¦¦à¦¨à¦•à¦¾à¦°à§€à¦•à§‡à¦‡ à¦¯à§‡à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦¶à§à¦¨à¦¾à¦¨à§€à¦¤à§‡ à¦¸à¦•à¦² à¦†à¦¬à§‡à¦¦à¦¨à¦•à¦¾à¦°à§€à¦•à§‡ à¦¯à§‡à¦¤à§‡ à¦¹à¦¬à§‡ à¦¨à¦¾à¦¤à§‹?"
    ],
    "namjari_status_check": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦¾à¦®à¦²à¦¾ à¦•à¦¿ à¦…à¦¬à¦¸à§à¦¥à¦¾à¦¯à¦¼ à¦†à¦›à§‡ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦œà¦¾à¦¨à¦¬à§‹?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦†à¦¬à§‡à¦¦à¦¨ à¦•à¦°à§‡à¦›à¦¿ à¦…à¦¨à§‡à¦•à¦¦à¦¿à¦¨ à¦•à¦¿ à¦†à¦¬à¦¸à§à¦¥à¦¾à¦¯à¦¼ à¦†à¦›à§‡ à¦œà¦¾à¦¨à¦¬à§‹ à¦•à¦¿à¦­à¦¾à¦¬à§‡?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦¾à¦®à¦²à¦¾à¦° à¦¸à§à¦Ÿà§à¦¯à¦¾à¦Ÿà¦¾à¦¸ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦œà¦¾à¦¨à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‹?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦¾à¦®à¦²à¦¾à¦° à¦…à¦¬à¦¸à§à¦¥à¦¾ à¦œà¦¾à¦¨à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦•à¦¿?"
    ],
    "namjari_rejected_appeal": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦¾à¦®à¦²à¦¾ à¦¨à¦¾à¦®à¦à§à¦œà§à¦° à¦¹à¦²à§‡ à¦•à¦¿ à¦•à¦°à¦¬à§‹?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦¾à¦®à¦²à¦¾ à¦¨à¦¾à¦®à¦à§à¦œà§à¦° à¦¹à¦¯à¦¼à§‡à¦›à§‡ à¦•à§‹à¦¨ à¦†à¦¦à¦¾à¦²à¦¤à§‡ à¦¯à¦¾à¦¬à§‹?", 
        "à¦¨à¦¾à¦®à¦à§à¦œà§à¦° à¦¹à¦²à§‡ à¦†à¦¬à¦¾à¦° à¦•à¦¿ à¦†à¦¬à§‡à¦¦à¦¨ à¦•à¦°à¦¬à§‹?",
        "à¦à¦¸à¦¿ (à¦²à§à¦¯à¦¾à¦¨à§à¦¡) à¦¶à§à¦¨à¦¾à¦¨à§€ à¦¨à¦¾ à¦¨à¦¿à¦¯à¦¼à§‡ à¦¨à¦¾à¦®à¦à§à¦œà§à¦° à¦•à¦°à§‡à¦›à§‡à¦¨?"
    ],
    "namjari_khatian_copy": [
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦à§à¦œà§à¦° à¦¹à¦¯à¦¼à§‡à¦›à§‡ à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨ à¦•à¦ªà¦¿ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦ªà§‡à¦¤à§‡ à¦ªà¦¾à¦°à¦¿?",
        "à¦†à¦®à¦¾à¦° à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦à§à¦œà§à¦° à¦¹à¦¯à¦¼à§‡à¦›à§‡ à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨ à¦•à¦¿ à¦†à¦®à¦¿ à¦‰à¦ à¦¾à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‹?",
        "à¦…à¦¨à¦²à¦¾à¦‡à¦¨à§‡ à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à§‡à¦›à¦¿ à¦†à¦®à¦¾à¦° à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨à§‡à¦° à¦•à¦ªà¦¿ à¦•à§‹à¦¥à¦¾à¦¯à¦¼ à¦ªà¦¾à¦¬à§‹?",
        "à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦®à¦à§à¦œà§à¦° à¦¹à¦²à§‡ à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨à§‡à¦° à¦ªà§à¦°à¦¿à¦¨à§à¦Ÿ à¦•à¦ªà¦¿ à¦¨à¦¿à¦œà§‡ à¦•à¦¿ à¦¨à¦¿à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‹?"
    ],
    "namjari_khatian_correction": [
        "à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨à§‡ à¦­à§à¦² à¦†à¦›à§‡ à¦¸à¦‚à¦¶à§‹à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?",
        "à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨à§‡ à¦¨à¦¾à¦® à¦­à§à¦² à¦²à§‡à¦–à¦¾ à¦¸à¦‚à¦¶à§‹à¦§à¦¨ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦•à¦°à¦¬à§‹?",
        "à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨à§‡ à¦œà¦®à¦¿à¦° à¦ªà¦°à¦¿à¦®à¦¾à¦£ à¦­à§à¦² à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦¸à¦‚à¦¶à§‹à¦§à¦¨ à¦•à¦°à¦¬à§‹?",
        "à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨ à¦¸à¦‚à¦¶à§‹à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦•à¦¿ à¦•à¦¾à¦—à¦œ à¦²à¦¾à¦—à§‡?",
        "à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨ à¦¸à¦‚à¦¶à§‹à¦§à¦¨ à¦•à¦°à¦¤à§‡ à¦•à¦¤ à¦Ÿà¦¾à¦•à¦¾ à¦²à¦¾à¦—à§‡?"
    ]
}

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def clean_question(text):
    """Basic cleaning of question text."""
    return text.strip().replace('"', '').replace("'", '')

def display_generated_text_streaming(generated_text, tag):
    """Display generated text with streaming effect and return cleaned lines."""
    print(f"ğŸ“º Generated content for {tag}:")
    print("=" * 80)
    
    lines = generated_text.strip().split('\n')
    cleaned_lines = []
    
    for line in lines:
        clean_line = clean_question(line)
        if len(clean_line) > 5 and any('\u0980' <= char <= '\u09FF' for char in clean_line):
            print(f"âœ¨ {clean_line}")
            cleaned_lines.append(clean_line)
            time.sleep(0.1)  # Small delay for streaming effect
    
    print("=" * 80)
    return cleaned_lines

def generate_questions_for_tag(tag, seed_questions, target_count):
    """Generate questions for a tag and stream to file."""
    
    # Create output directory
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    # Create file and write header immediately
    filename = f"{tag}.csv"
    filepath = os.path.join(OUTPUT_DIR, filename)
    
    print(f"\nğŸ·ï¸  Processing: {tag}")
    print(f"ğŸ“ Creating: {filepath}")
    print(f"ğŸ¯ Target: {target_count} questions")
    
    with open(filepath, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f, quoting=csv.QUOTE_ALL)
        writer.writerow(['question', 'tag'])
        
        # Write seed questions first  
        print(f"ğŸ“ Writing {len(seed_questions)} seed questions...")
        for i, seed in enumerate(seed_questions, 1):
            clean_q = clean_question(seed)
            writer.writerow([clean_q, tag])
            f.flush()
            print(f"  {i:2d}. {clean_q}")
        
        seed_count = len(seed_questions)
        print(f"âœ… Wrote {seed_count} seed questions")
        
        # Generate more if needed
        questions_needed = target_count - seed_count
        if questions_needed <= 0:
            print(f"âœ… Target reached with seeds only")
            return filepath
        
        print(f"ğŸ¤– Generating {questions_needed} new questions...")
        
        # Create simple prompt
        examples = "\n".join(seed_questions)
        
        prompt = f"""Generate {questions_needed} new Bengali questions about '{tag}' similar to these examples:

{examples}

Requirements:
- Write in natural Bengali like the examples
- One question per line
- No numbers or bullets  
- Different question types (à¦•à¦¿à¦­à¦¾à¦¬à§‡, à¦•à¦¿, à¦•à§‹à¦¥à¦¾à¦¯à¦¼, etc.)
- Use colloquial language like examples

Generate {questions_needed} questions now:"""

        try:
            print("ğŸ”„ Calling OpenAI...")
            
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that generates Bengali questions about land registration (namjari) topics. Generate only the questions, one per line, with no numbering or bullets."},
                    {"role": "user", "content": prompt}
                ],
                max_completion_tokens=8000  # More tokens for gpt-5 reasoning + output
            )
            
            # Extract and display generated text with streaming
            generated_text = response.choices[0].message.content
            print(f"ğŸ“¡ Generated text length: {len(generated_text) if generated_text else 0} characters")
            
            # Debug: print response details if empty
            if not generated_text or len(generated_text.strip()) == 0:
                print(f"âš ï¸  Empty response detected!")
                print(f"ğŸ” Response object: {response}")
                print(f"ğŸ” Response choices: {len(response.choices) if response.choices else 0}")
                if response.choices and len(response.choices) > 0:
                    print(f"ğŸ” First choice: {response.choices[0]}")
                    print(f"ğŸ” Message content: '{response.choices[0].message.content}'")
            
            # Display generated text with streaming effect and get cleaned lines
            cleaned_lines = display_generated_text_streaming(generated_text, tag)
            
            # Add cleaned questions to CSV
            added_count = 0
            print(f"ğŸ“ Writing {len(cleaned_lines)} valid questions to CSV...")
            
            for clean_q in cleaned_lines:
                writer.writerow([clean_q, tag])
                f.flush()
                added_count += 1
                print(f"âœ… Written to CSV: {clean_q[:50]}..." if len(clean_q) > 50 else f"âœ… Written to CSV: {clean_q}")
                
                if seed_count + added_count >= target_count:
                    break
            
            print(f"âœ… Added {added_count} new questions")
            print(f"ğŸ“Š Total in file: {seed_count + added_count}")
            
        except Exception as e:
            print(f"âŒ Error generating questions: {e}")
            print(f"ğŸ“Š File contains {seed_count} seed questions only")
            print(f"ğŸ”„ You can run the script again to retry generation for this tag")
    
    return filepath

def main():
    print("ğŸš€ Namjari Question Generator - PRODUCTION MODE (All 13 Tags)")
    print("=" * 80)
    print(f"ğŸ¯ Target: {TARGET_ROWS} questions across 13 tags")
    print(f"ğŸ“º Will show all generated questions streaming!")
    print(f"ğŸ”§ Model: {MODEL}")
    print(f"ğŸ“ Output: {OUTPUT_DIR}/ (individual files)")
    print("=" * 80)
    
    # Process all tags
    all_tags = list(SEED_DATA.keys())
    tags = all_tags  # All 13 tags for production
    
    # Calculate questions per tag
    base_count = TARGET_ROWS // len(tags)
    remainder = TARGET_ROWS % len(tags)
    
    print(f"ğŸ“Š Distribution: {base_count} questions per tag")
    if remainder > 0:
        print(f"ğŸ“Š Extra: {remainder} tags will get +1 question")
    
    files_created = []
    
    start_time = time.time()
    
    for i, tag in enumerate(tags):
        target = base_count + (1 if i < remainder else 0)
        seeds = SEED_DATA[tag]
        
        print(f"\n{'='*80}")
        print(f"[{i+1}/{len(tags)}] Processing: {tag}")
        print(f"ğŸ¯ Target: {target} questions ({len(seeds)} seeds + {target-len(seeds)} new)")
        print(f"{'='*80}")
        
        tag_start_time = time.time()
        filepath = generate_questions_for_tag(tag, seeds, target)
        tag_duration = time.time() - tag_start_time
        
        # Count actual questions in file
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                actual_count = sum(1 for _ in reader)
        except Exception as e:
            print(f"âš ï¸ Warning: Could not count questions in file: {e}")
            actual_count = len(seeds)
            
        files_created.append((tag, filepath, actual_count, tag_duration))
        
        print(f"ğŸ‰ Completed: {os.path.basename(filepath)} ({actual_count} questions)")
        print(f"â±ï¸  Time taken: {tag_duration:.1f} seconds")
        print(f"ğŸ“¡ Monitor: tail -f {filepath}")
    
    # Summary
    total_duration = time.time() - start_time
    total_questions = sum(count for _, _, count, _ in files_created)
    
    print(f"\nğŸ† ALL DONE!")
    print("=" * 80)
    print(f"ğŸ“ Directory: {OUTPUT_DIR}/")
    print(f"ğŸ“„ Files: {len(files_created)}")
    print(f"ğŸ“Š Total questions: {total_questions}")
    print(f"â±ï¸  Total time: {total_duration:.1f} seconds")
    print(f"âš¡ Average: {total_questions/total_duration:.1f} questions/second")
    print("=" * 80)
    
    print(f"\nğŸ“‹ Detailed Results:")
    print(f"{'Tag':<30} {'File':<35} {'Questions':<10} {'Time(s)':<8}")
    print("-" * 85)
    for tag, filepath, count, duration in files_created:
        filename = os.path.basename(filepath)
        print(f"{tag:<30} {filename:<35} {count:<10} {duration:<8.1f}")
    
    print(f"\nğŸ“¡ Monitor commands:")
    print("   ls -la namjari_questions/")
    print("   wc -l namjari_questions/*.csv")
    print("   find namjari_questions/ -name '*.csv' -exec wc -l {} + | sort -n")
    
    print(f"\nğŸ¯ Mission Accomplished!")
    print(f"   Generated {total_questions} Bengali questions across {len(files_created)} namjari topics!")
    print(f"   Ready for dataset training! ğŸš€")

if __name__ == "__main__":
    main()
